# For reference: https://pytorch.org/serve/configuration.html

# default setting
inference_address=http://0.0.0.0:8082
management_address=http://0.0.0.0:8083
metrics_address=http://0.0.0.0:8084
number_of_netty_threads=32
job_queue_size=1000
model_store=/home/model-server/model-store
workflow_store=/home/model-server/wf-store

min_worker=2

#Don't use this for prod, slows initialization down
# install_py_dep_per_model=true

# cors_allowed_origin is required to enable CORS, use '*' or your domain name
cors_allowed_origin='*'
# required if you want to use preflight request
cors_allowed_methods=GET, POST, PUT, OPTIONS
# required if the request has an Access-Control-Request-Headers header
# cors_allowed_headers=X-Custom-Header

# set for batch inference
max_batch_delay=5000
batch_size=4
default_response_timeout=800
vmargs=-Dlog4j.configurationFile=file:///home/model-server/worker/log4j2.xml
max_response_size=1000000000
max_request_size=1000000000